{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, Dropout\n",
    "from keras.applications.xception import Xception\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import sys, gc, os\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDC_models.audio_models import panotti_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = panotti_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa import amplitude_to_db\n",
    "from librosa.feature import melspectrogram\n",
    "\n",
    "def compute_melgram(audio):\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 44100\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "\n",
    "    ret = amplitude_to_db(\n",
    "        melspectrogram(\n",
    "            y = audio,\n",
    "            sr = SR,\n",
    "            hop_length = HOP_LEN,\n",
    "            n_fft = N_FFT, \n",
    "            n_mels = N_MELS) **2,\n",
    "        ref_power = 1.0)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/cifar10_resnet/\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train = np.empty(shape=(0,d,d,3), dtype=np.int8)\n",
    "labels = np.empty(shape=(0,2), dtype=np.float)\n",
    "\n",
    "for f in glob.glob(\"../data/train_Xception_*\"):\n",
    "    print(f)\n",
    "    train_loader = np.load(f)\n",
    "    fakes = train_loader['fakes']\n",
    "    train = np.append(train,fakes,axis=0)\n",
    "    print('fakes',len(fakes))\n",
    "    reals = train_loader['reals']\n",
    "    train = np.append(train,reals,axis=0)\n",
    "    print('reals',len(reals))\n",
    "    l = np.append(np.full(len(fakes),'FAKE'),np.full(len(reals),'REAL'))\n",
    "    l = le.fit_transform(l)\n",
    "    l = np_utils.to_categorical(l, 2)\n",
    "    print('labels',labels.ndim,l.ndim)\n",
    "    labels = np.append(labels,l,axis=0)\n",
    "    \n",
    "print('train=',len(train))\n",
    "print('labels',len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image dimensions.\n",
    "input_shape = train[1].shape\n",
    "print(input_shape)\n",
    "num_classes=2\n",
    "# Training parameters\n",
    "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
    "epochs = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xception1():\n",
    "    image_input = Input(shape=(d,d,3))\n",
    "    \n",
    "    cnn = Xception(include_top=False, weights='imagenet', input_tensor=None, input_shape=input_shape, pooling='avg')(image_input)\n",
    "\n",
    "    d1 = Dense(512)(cnn)\n",
    "    d1 = Dropout(0.5)(d1)\n",
    "  \n",
    "    output = Dense(2, activation='softmax')(d1)\n",
    "    \n",
    "    model = Model(inputs=image_input, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Xception1()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Prepare model model saving directory.\n",
    "save_dir = '../models'\n",
    "model_name = 'DDC_Xception_model.{epoch:03d}.h5' \n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.2,\n",
    "                               cooldown=0,\n",
    "                               patience=2,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          patience=6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler, earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(train, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(trainX,\n",
    "              trainY,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(testX, testY),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 8))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax1.plot(history.history['acc'])\n",
    "ax1.plot(history.history['val_acc'])\n",
    "ax1.set_title('Model accuracy')\n",
    "ax1.set(xlabel=\"Epoch\", ylabel=\"Accuracy\")\n",
    "ax1.legend(['Train', 'Test'], loc='lower right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax2.plot(history.history['loss'])\n",
    "ax2.plot(history.history['val_loss'])\n",
    "ax2.set_title('Model loss')\n",
    "ax2.set(xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "ax2.legend(['Train', 'Test'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.log_loss(testY[:,0],predictions[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
