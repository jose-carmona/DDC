{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoProcessor:\n",
    "    def __init__(self, \n",
    "                 modelFile=\"../models/res10_300x300_ssd_iter_140000.caffemodel\",\n",
    "                 configFile = \"../models/deploy.prototxt\",\n",
    "                 landmarkFile = \"../models/shape_predictor_68_face_landmarks.dat\",\n",
    "                 right_eye_file ='../models/haarcascade_righteye_2splits.xml',\n",
    "                 left_eye_file ='../models/haarcascade_lefteye_2splits.xml',\n",
    "                 mouth_file ='../models/haarcascade_smile.xml',\n",
    "                 max_interations = 10,\n",
    "                 conf_threshold = 0.60,\n",
    "                 normalized_dim = 500):\n",
    "        self.modelFile = modelFile\n",
    "        self.configFile = configFile\n",
    "        self.max_interations = max_interations\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.normalized_dim = normalized_dim\n",
    "        self.net = cv2.dnn.readNetFromCaffe(self.configFile, self.modelFile)\n",
    "        self.right_eye_casc = cv2.CascadeClassifier(right_eye_file)\n",
    "        self.left_eye_casc = cv2.CascadeClassifier(left_eye_file)\n",
    "        self.mouth_casc = cv2.CascadeClassifier(mouth_file)\n",
    "        self.mean = 128\n",
    "        self.min_YCrCb = np.array([0,133,77],np.uint8)\n",
    "        self.max_YCrCb = np.array([235,173,127],np.uint8)\n",
    "        self.h = 260\n",
    "        self.w = 208\n",
    "        \n",
    "        #self.min_HSV = np.array([0, 48, 80], dtype = \"uint8\")\n",
    "        #self.max_HSV = np.array([20, 255, 255], dtype = \"uint8\")\n",
    "        self.min_HSV = np.array([0, 58, 30], dtype = \"uint8\")\n",
    "        self.max_HSV = np.array([33, 255, 255], dtype = \"uint8\")\n",
    "\n",
    "    def extract_face_box(self, img):\n",
    "        (h, w) = img.shape[:2]\n",
    "        face_box = None\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (103.93, 116.77, 123.68))\n",
    "        self.net.setInput(blob)\n",
    "        detections = self.net.forward()\n",
    "        for i in range(detections.shape[2]):\n",
    "            if detections[0, 0, i, 2] > self.conf_threshold:\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                iw = box[2] - box[0]\n",
    "                ih = box[3] - box[1]\n",
    "                if ih > 100 and ih < 600 and iw > 50 and iw < 500:\n",
    "                    face_box = box.astype(\"int\")\n",
    "                    break\n",
    "                    \n",
    "        return face_box\n",
    "\n",
    "    def extract_random_faces(self, filename, nframe=100, num_faces=1):\n",
    "        captured_faces = []\n",
    "        iterations = 0\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_length = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        while len(captured_faces) < num_faces and iterations < self.max_interations:\n",
    "            iterations += 1\n",
    "            # v_cap.set(1, np.random.randint(v_length)-1)\n",
    "            v_cap.set(1, nframe)\n",
    "            \n",
    "            ret, img = v_cap.read()\n",
    "        \n",
    "            if ret == True:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                (h, w) = img.shape[:2]\n",
    "                blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (103.93, 116.77, 123.68))\n",
    "                self.net.setInput(blob)\n",
    "                detections = self.net.forward()\n",
    "                for i in range(detections.shape[2]):\n",
    "                    confidence = detections[0, 0, i, 2]            \n",
    "                    if confidence > self.conf_threshold:\n",
    "                        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "                        face = img[y1:y2, x1:x2]\n",
    "                        \n",
    "                        # normlize\n",
    "                        # face = cv2.resize(face, self.normalized_dim)\n",
    "                        captured_faces.append(face)\n",
    "        \n",
    "        # When everything done, release the video capture and video write objects\n",
    "        v_cap.release()\n",
    "    \n",
    "        return captured_faces\n",
    "\n",
    "\n",
    "    def right_eye(self, face):\n",
    "        right_eye = self.right_eye_casc.detectMultiScale(\n",
    "            face,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=4,\n",
    "            flags=cv2.CASCADE_FIND_BIGGEST_OBJECT)\n",
    "        return right_eye[0]\n",
    "\n",
    "    def left_eye(self, face):\n",
    "        right_eye = self.left_eye_casc.detectMultiScale(\n",
    "            face,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=4,\n",
    "            flags=cv2.CASCADE_FIND_BIGGEST_OBJECT)\n",
    "        return right_eye[0]\n",
    "\n",
    "    def mouth(self, face):\n",
    "        right_eye = self.mouth_casc.detectMultiScale(\n",
    "            face,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=4,\n",
    "            flags=cv2.CASCADE_FIND_BIGGEST_OBJECT)\n",
    "        return right_eye[0]\n",
    "\n",
    "\n",
    "    def extract_diff(self, filename, nframe = 42, num_diff = 1):\n",
    "        captured_diff = np.empty(shape=(0,d,d,3), dtype=np.int8)\n",
    "        iterations = 0\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_length = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        while len(captured_diff) < num_diff and iterations < self.max_interations:\n",
    "            iterations += 1\n",
    "            v_cap.set(1, nframe)\n",
    "            ret, img_base = v_cap.read()\n",
    "            if ret == True:\n",
    "                v_cap.set(1, nframe + self.nframesdiff)\n",
    "                ret, img = v_cap.read()\n",
    "                \n",
    "            if ret == True:\n",
    "                img_base = cv2.cvtColor(img_base, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                face1 = cv2.resize(self.extract_face(img_base), (self.normalized_dim,self.normalized_dim))\n",
    "                face2 = cv2.resize(self.extract_face(img), (self.normalized_dim,self.normalized_dim))\n",
    "                if type(face1) == type(face2):\n",
    "                    face_diff = cv2.absdiff(face2,face1)\n",
    "                    # face_diff = cv2.absdiff(self.mean,face_diff)\n",
    "                    face_diff = cv2.normalize(face_diff,None,0,255,cv2.NORM_MINMAX)\n",
    "                    if face_diff is not None:\n",
    "                        captured_diff = np.append(captured_diff,[face_diff],axis=0)\n",
    "        \n",
    "        # When everything done, release the video capture and video write objects\n",
    "        v_cap.release()\n",
    "        \n",
    "        # Closes all the frames\n",
    "        cv2.destroyAllWindows() \n",
    "\n",
    "        return captured_diff\n",
    "\n",
    "    def extract_motion(self, filename, nframe = 42, num_motions = 1):\n",
    "        captured_motions = np.empty(shape=(0,d,d,3), dtype=np.int8)\n",
    "        hsv = np.empty(shape=(d,d,3), dtype=np.uint8)\n",
    "        iterations = 0\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_length = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        while len(captured_motions) < num_motions and iterations < self.max_interations:\n",
    "            iterations += 1\n",
    "            v_cap.set(1, nframe)\n",
    "            ret, img_base = v_cap.read()\n",
    "            if ret == True:\n",
    "                v_cap.set(1, nframe + self.nframesdiff)\n",
    "                ret, img = v_cap.read()\n",
    "                \n",
    "            if ret == True:\n",
    "                img_base = cv2.cvtColor(img_base, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                face1 = cv2.resize(self.extract_face(img_base), (self.normalized_dim,self.normalized_dim))\n",
    "                face2 = cv2.resize(self.extract_face(img), (self.normalized_dim,self.normalized_dim))\n",
    "                \n",
    "                if type(face1) == type(face2):\n",
    "                    face1 = cv2.cvtColor(face1, cv2.COLOR_RGB2GRAY)\n",
    "                    face2 = cv2.cvtColor(face2, cv2.COLOR_RGB2GRAY)\n",
    "                    hsv[...,1] = 255\n",
    "\n",
    "                    flow = cv2.calcOpticalFlowFarneback(face1, face2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "                    hsv[...,0] = ang*180/np.pi/2\n",
    "                    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "                    bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2RGB)\n",
    "                    if bgr is not None:\n",
    "                        captured_motions = np.append(captured_motions,[bgr],axis=0)\n",
    "        \n",
    "        # When everything done, release the video capture and video write objects\n",
    "        v_cap.release()\n",
    "        \n",
    "        # Closes all the frames\n",
    "        cv2.destroyAllWindows() \n",
    "\n",
    "        return captured_motions\n",
    "\n",
    "    def extract_skin(self, image):\n",
    "        imageYCrCb = cv2.cvtColor(image,cv2.COLOR_RGB2YCR_CB)\n",
    "        skinRegionYCrCb = cv2.inRange(imageYCrCb,self.min_YCrCb,self.max_YCrCb)\n",
    "        return (skinRegionYCrCb/255).sum(), cv2.bitwise_and(image, image, mask = skinRegionYCrCb)\n",
    "\n",
    "    def extract_skin_hue(self, image):\n",
    "        imagehsv = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "        skinRegionhsv = cv2.inRange(imagehsv,self.min_HSV,self.max_HSV)\n",
    "        return cv2.bitwise_and(image, image, mask = skinRegionhsv)\n",
    "\n",
    "    def extract(self, filename, output):\n",
    "        nfaces = 0\n",
    "        last_face_box = None\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        v_out = cv2.VideoWriter(output,fourcc, 30, (2*self.normalized_dim,self.normalized_dim))\n",
    "\n",
    "        while True:\n",
    "            ret, img = v_cap.read()\n",
    "            if ret != True:\n",
    "                break\n",
    "            \n",
    "            # extract face\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            face_box = self.extract_face_box(img)\n",
    "\n",
    "            if face_box is None and last_face_box is not None:\n",
    "                face_box = last_face_box\n",
    "            \n",
    "            if face_box is not None:\n",
    "                z = np.zeros((self.normalized_dim,self.normalized_dim,3),dtype=\"uint8\")\n",
    "                (x1, y1, x2, y2) = face_box\n",
    "                (x1, y1, x2, y2) = ((x1+x2)//2-self.w//2, (y1+y2)//2-self.h//2,\n",
    "                                    (x1+x2)//2+self.w//2, (y1+y2)//2+self.h//2)\n",
    "                (npixels,face) = self.extract_skin(img[y1:y2, x1:x2])\n",
    "                # face = cv2.resize(face, (self.normalized_dim,self.normalized_dim))\n",
    "                (u1,u2,u3) = face.shape\n",
    "                z[0:u1,0:u2,0:u3] = face\n",
    "                frame = cv2.cvtColor(z, cv2.COLOR_RGB2BGR)\n",
    "                last_face_box = face_box\n",
    "                nfaces += 1\n",
    "            else:\n",
    "                frame = np.zeros((self.normalized_dim,self.normalized_dim,3),dtype=\"uint8\")\n",
    "\n",
    "            fig = Figure(figsize=(5, 5), dpi=100)\n",
    "            canvas = FigureCanvasAgg(fig)\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "            color = ('b','y','r')\n",
    "            face_hsv = cv2.cvtColor(face,cv2.COLOR_RGB2HSV)\n",
    "            for i,col in enumerate(color):\n",
    "                hist = cv2.calcHist([face_hsv],[i],None,[256],[0,256])/npixels\n",
    "                ax.plot(hist,color = col)\n",
    "                ax.set_ylim([.0, .3])\n",
    "\n",
    "            canvas.draw()\n",
    "            s, (width, height) = canvas.print_to_buffer()\n",
    "            im = Image.frombytes(\"RGBA\", (width, height), s)\n",
    "            fhist = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)\n",
    "            v_out.write(np.hstack([frame,fhist]))\n",
    "                \n",
    "        v_out.release()\n",
    "        v_cap.release()\n",
    "        \n",
    "        # Closes all the frames\n",
    "        cv2.destroyAllWindows() \n",
    "        print(f'faces = {nfaces}')\n",
    "\n",
    "    def extract_lstm1(self, filename):\n",
    "        sequence = []\n",
    "        nfaces = 0\n",
    "        last_face_box = None\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "\n",
    "        while nfaces < 60:\n",
    "            ret, img = v_cap.read()\n",
    "            if ret != True:\n",
    "                break\n",
    "            \n",
    "            # extract face\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            face_box = self.extract_face_box(img)\n",
    "\n",
    "            if face_box is None and last_face_box is not None:\n",
    "                face_box = last_face_box\n",
    "            \n",
    "            if face_box is not None:\n",
    "                (x1, y1, x2, y2) = face_box\n",
    "                (x1, y1, x2, y2) = ((x1+x2)//2-self.w//2, (y1+y2)//2-self.h//2,\n",
    "                                    (x1+x2)//2+self.w//2, (y1+y2)//2+self.h//2)\n",
    "                if x1 < 0:\n",
    "                    x1 = 0\n",
    "                    x2 = self.w\n",
    "                if y1 < 0:\n",
    "                    y1 = 0\n",
    "                    y2 = self.h\n",
    "                try:\n",
    "                    (npixels,face) = self.extract_skin(img[y1:y2, x1:x2])\n",
    "                except:\n",
    "                    print(filename)\n",
    "                    print(y1,y2,x1,x2)\n",
    "                    raise\n",
    "                face_hsv = cv2.cvtColor(face,cv2.COLOR_RGB2HSV)\n",
    "                hist = cv2.calcHist([face_hsv],[0],None,[256],[0,256])/npixels\n",
    "                sequence.append(hist.flatten())\n",
    "                last_face_box = face_box\n",
    "                nfaces += 1\n",
    "                \n",
    "        v_cap.release()\n",
    "        \n",
    "        # Closes all the frames\n",
    "        cv2.destroyAllWindows() \n",
    "\n",
    "        # 60 frames --> hist[hue]\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfdc_train_part_21\n",
      "dfdc_train_part_20\n",
      "dfdc_train_part_1\n",
      "dfdc_train_part_14\n",
      "dfdc_train_part_48\n",
      "dfdc_train_part_3\n",
      "dfdc_train_part_2\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for root, dirs, files in os.walk('../videos', topdown=False):\n",
    "    for name in dirs:\n",
    "        print(name)\n",
    "        dfdir = pd.read_json('../videos/' + name + '/metadata.json')\n",
    "        dfdir = dfdir.T\n",
    "        dfdir['dir'] = name\n",
    "        df = df.append(dfdir)\n",
    "df[\"processed\"] = 'False'\n",
    "\n",
    "df.to_csv('../data/metadata_lstm1.csv',index_label='video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/metadata_lstm1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "      <th>split</th>\n",
       "      <th>dir</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8898</th>\n",
       "      <td>gnnuktzwwy.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>fsvnrnvmfa.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>wbalhrcaxc.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>hfiizhatus.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8794</th>\n",
       "      <td>ocwkmbqfuh.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>uuyyezfcnx.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11566</th>\n",
       "      <td>jafxtknnoa.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>ofdhwdwvqb.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9020</th>\n",
       "      <td>lofyajpsll.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>iwrkkcfcob.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>jaurdjnhfi.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>mufzpbhkeb.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>wbpnikwbii.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>aucdquhxmk.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7806</th>\n",
       "      <td>zxjoffxykl.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>uvixoukpiv.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10158</th>\n",
       "      <td>calqcklglq.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>eqabuehals.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9512</th>\n",
       "      <td>mzovnxbrnd.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>uydlsvuore.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                video label        original  split                 dir  processed\n",
       "8898   gnnuktzwwy.mp4  FAKE  fsvnrnvmfa.mp4  train  dfdc_train_part_48      False\n",
       "2380   wbalhrcaxc.mp4  FAKE  hfiizhatus.mp4  train  dfdc_train_part_20      False\n",
       "8794   ocwkmbqfuh.mp4  FAKE  uuyyezfcnx.mp4  train  dfdc_train_part_48      False\n",
       "11566  jafxtknnoa.mp4  FAKE  ofdhwdwvqb.mp4  train   dfdc_train_part_3      False\n",
       "9020   lofyajpsll.mp4  FAKE  iwrkkcfcob.mp4  train  dfdc_train_part_48      False\n",
       "6428   jaurdjnhfi.mp4  FAKE  mufzpbhkeb.mp4  train  dfdc_train_part_14      False\n",
       "2643   wbpnikwbii.mp4  FAKE  aucdquhxmk.mp4  train  dfdc_train_part_20      False\n",
       "7806   zxjoffxykl.mp4  FAKE  uvixoukpiv.mp4  train  dfdc_train_part_14      False\n",
       "10158  calqcklglq.mp4  FAKE  eqabuehals.mp4  train  dfdc_train_part_48      False\n",
       "9512   mzovnxbrnd.mp4  FAKE  uydlsvuore.mp4  train  dfdc_train_part_48      False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('label==\"FAKE\" and processed == False').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:55<00:00,  3.36s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:53<00:00,  3.24s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:00<00:00,  3.23s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:54<00:00,  3.26s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:47<00:00,  3.25s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:43<00:00,  3.10s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:42<00:00,  3.14s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:47<00:00,  3.36s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:03<00:00,  5.73s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:42<00:00,  3.25s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:45<00:00,  2.98s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:56<00:00,  3.50s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:45<00:00,  3.27s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:44<00:00,  3.31s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:42<00:00,  3.09s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 15/32 [00:49<00:53,  3.15s/it]/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:291: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:291: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 32/32 [02:16<00:00,  3.42s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:43<00:00,  3.15s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:42<00:00,  3.29s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:55<00:00,  3.35s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:47<00:00,  3.18s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:08<00:00,  3.34s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:02<00:00,  3.28s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0e85cd018813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mseq_fakes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_lstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../videos/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_fakes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfakes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfakes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_fakes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3c0c91004dc3>\u001b[0m in \u001b[0;36mextract_lstm1\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;31m# extract face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mface_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_face_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mface_box\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlast_face_box\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3c0c91004dc3>\u001b[0m in \u001b[0;36mextract_face_box\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m103.93\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m116.77\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m123.68\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch = 13\n",
    "nsample = 32\n",
    "d = 128\n",
    "\n",
    "vp = VideoProcessor()\n",
    "\n",
    "while True:\n",
    "    sample = df.query('label==\"FAKE\" and processed == False').sample(nsample)\n",
    "    if len(sample) == 0:\n",
    "        break\n",
    "\n",
    "    fakes = np.empty(shape=(0,60,256), dtype=np.float32)\n",
    "    reals = np.empty(shape=(0,60,256), dtype=np.float32)\n",
    "\n",
    "    batch += 1\n",
    "    for index, row in tqdm(sample.iterrows(), total=nsample):\n",
    "        seq_fakes = vp.extract_lstm1('../videos/' + row.dir + '/' + row.video)\n",
    "        if len(seq_fakes) == 60:\n",
    "            fakes = np.append(fakes,np.expand_dims(seq_fakes, axis=0),axis=0)\n",
    "        seq_reals = vp.extract_lstm1('../videos/' + row.dir + '/' + row.original)\n",
    "        if len(seq_reals) == 60:\n",
    "            reals = np.append(reals,np.expand_dims(seq_reals, axis=0),axis=0)\n",
    "\n",
    "    np.savez(f'../data/train_lstm1_{batch}', fakes=fakes, reals=reals)\n",
    "    df.loc[sample.index,'processed'] = f'train_lstm1_{batch}'\n",
    "    df.to_csv('../data/metadata_lstm1.csv',index = False)\n",
    "    print(f'saved batch: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
